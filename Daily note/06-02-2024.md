Trying local LLM's 

Code llama is terrific when used the same way as Chat GPT, but running a web UI every time is a drag. I'm trying to mimmic copilot .

First thing i tried was a VS Code plugin called OllamaCopilot, it was some hot garbage, don't use it.

The next thing I tried was a plugin called Continue, its actually decent, uses command + M for chat and command + L for CLI-ish experience. 

Running all the LLMs on Ollama, MacBook Air M1

Codellama 7B (code and instruct)- Based on llama2 and itself baseline for the rest of the models, its not really worth the compute.

Deepseek-coder is the smallest model that works***, just 1,6B it is really fast, but in combination with Continue it keeps writing insane comments that outweigh the actual code 10 to 1. Im sure there is a way to mitigate this but not today.

Mistral 7B (text and instruct) works well for general chat, however the performance is not terrific because it tends to loop on it self. It attempts to write really complex code 200+ lines but seemingly forgets the things it already wrote and proceeds to write them again. Not worth it if you don't want to heat your house on electricity .

Stable-code 3B, most promising of the bunch contains both latest and code tags. It's not as fast as deepseek-coder but still works perfectly on 4Gb of RAM. Im not sure i'm using it correctly. It also loops but its faster then Mistral, I Tabbed out and came back to a 400 line code recommendation. There must be a way to fix this.